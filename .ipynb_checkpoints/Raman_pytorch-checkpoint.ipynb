{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the neccessary libraries and files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data libraries\n",
    "import shelve\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.fft import fft, ifft, ifftshift\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Machine learning libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets ,transforms\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Stefano's functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.interpolate as si"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = 1600\n",
    "num_th = 5\n",
    "peak_pos = []\n",
    "interpol_mse = []\n",
    "slide_win = 100\n",
    "nu = 0.01\n",
    "interpol_mse = interpol_mse + [1000, 1000, 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_merge(inter, start_index = 0):\n",
    "    for i in range(start_index, len(inter) - 1):\n",
    "        if inter[i][1] > inter[i+1][0]:\n",
    "            new_start = inter[i][0]\n",
    "            new_end = inter[i+1][1]\n",
    "            inter[i] = [new_start, new_end]\n",
    "            del inter[i+1]\n",
    "            return recursive_merge(inter.copy(), start_index=i)\n",
    "    return inter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(num_th):\n",
    "    # find the points that minimizes the variance of the data minus spline interpolation\n",
    "    # scan all points, who cares\n",
    "    idx_left=list(set(range(res)) - set(peak_pos))\n",
    "    while True:\n",
    "        plt.plot(x_data,y_data)\n",
    "        plt.plot(x_data[idx_left],y_data[idx_left],'*')\n",
    "    interpol_pos = []\n",
    "    interpol_pos =  [0,np.argmin(y_data[idx_left]),res-1]\n",
    "    #for i in range(int(y_data.shape[0]/slide_win)):\n",
    "    for i in range(2*int(y_data.shape[0]/slide_win)):\n",
    "        min_pos=0\n",
    "        y_hat = si.interp1d(x_data[interpol_pos],y_data[interpol_pos])(x_data)\n",
    "        min_mse=(1-nu)*np.dot((y_hat-y_data>0),np.abs((y_hat-y_data)))+nu*np.var(y_hat-y_data)\n",
    "        tmp_interpol_pos=list(set(range(int(i*(slide_win/2)),min(int(i*(slide_win/2)+(slide_win)),res))) - set(peak_pos))\n",
    "        for k in range(len(tmp_interpol_pos)):\n",
    "            tmp_pos=np.concatenate((interpol_pos,[tmp_interpol_pos[k]]))\n",
    "            y_hat = si.interp1d(x_data[tmp_pos],y_data[tmp_pos])(x_data)\n",
    "            # generalize to any loss\n",
    "            # tmp_mse=np.var(y_hat-y_data)\n",
    "            tmp_mse=(1-nu)*np.dot((y_hat-y_data>0),np.abs((y_hat-y_data)))+nu*np.var(y_hat-y_data)\n",
    "            # update the minimum\n",
    "            if tmp_mse<min_mse:\n",
    "                min_pos = tmp_interpol_pos[k]\n",
    "                min_mse = tmp_mse\n",
    "        interpol_pos.append(min_pos)\n",
    "        #interpol_pos.sort()\n",
    "        interpol_mse.append(min_mse)\n",
    "        unique_pos=np.array([int(interpol_pos.index(x)) for x in set(interpol_pos)])\n",
    "        interpol_pos=list(np.array(interpol_pos)[unique_pos.astype(int)])\n",
    "        interpol_mse=list(np.array(interpol_mse)[unique_pos.astype(int)])\n",
    "        # sort points\n",
    "        sort_pos=np.argsort(interpol_pos)\n",
    "        interpol_pos=list(np.array(interpol_pos)[sort_pos.astype(int)])\n",
    "        interpol_mse=list(np.array(interpol_mse)[sort_pos.astype(int)])\n",
    "        # remove points that are too close\n",
    "    y_hat = si.interp1d(x_data[interpol_pos],y_data[interpol_pos])(x_data)\n",
    "    y_bsp = np.poly1d(np.polyfit(x_data[interpol_pos], y_data[interpol_pos], 3) )(x_data)\n",
    "    #y_bsp=si.interp1d(x_data[interpol_pos],y_data[interpol_pos], kind='cubic')(x_data)\n",
    "    while True:\n",
    "        plt.plot(x_data,y_data)\n",
    "        plt.plot(x_data,y_hat)\n",
    "        plt.plot(x_data,y_bsp)\n",
    "    mean_level=y_bsp\n",
    "    #--------------------------------------------------------------------\n",
    "    #   find when you acre over the mean level\n",
    "    #--------------------------------------------------------------------\n",
    "    # th=2*np.sqrt(np.var(y_data-mean_level))\n",
    "    th=np.sqrt(np.var(y_data-mean_level))\n",
    "    pos=np.array(np.where((y_data-mean_level)<th)[0])\n",
    "    while True:\n",
    "        plt.plot(x_data,y_data)\n",
    "        plt.plot(x_data,y_bsp)\n",
    "        plt.plot(x_data[pos],y_bsp[pos],'*')\n",
    "    #--------------------------------------------------------------------\n",
    "    #   merge the points\n",
    "    #--------------------------------------------------------------------\n",
    "    diff_pos=pos[1:]-pos[:-1]-1\n",
    "    jumps=np.where(diff_pos>0)[0]\n",
    "    #if the final element is res, the add a jomp an the end\n",
    "    if pos[-1]==res-1:\n",
    "        jumps=np.append(jumps,pos.shape[0]-1)\n",
    "    final_lb=[]\n",
    "    final_rb=[]\n",
    "    if jumps.size==0:\n",
    "            final_lb.append(pos[0])\n",
    "            final_rb.append(pos[-1])\n",
    "    else:\n",
    "            final_lb.append(pos[0])\n",
    "            final_rb.append(pos[jumps[0]])\n",
    "            k=0\n",
    "            while k<jumps.shape[0]-1:\n",
    "                #\n",
    "                final_lb.append(pos[jumps[k]+1])\n",
    "                # go to the next gap\n",
    "                k=k+1\n",
    "                final_rb.append(pos[jumps[k]])\n",
    "    # add the first and the last intervals\n",
    "    idx_lr=np.zeros([2,len(final_rb)])\n",
    "    idx_lr[0]=np.array(final_lb)\n",
    "    idx_lr[1]=np.array(final_rb)\n",
    "    idx_lr.astype(int)\n",
    "    idx_lr=idx_lr.T\n",
    "    # merge intervals\n",
    "    # remove the one intervals\n",
    "    idx_lr=idx_lr[np.where(idx_lr[:,1]-idx_lr[:,0]>2)[0],:]\n",
    "    merged = recursive_merge(idx_lr.tolist())\n",
    "    idx_lr = np.array(merged).astype(int)\n",
    "    idx_lr_poly = idx_lr\n",
    "for j in range(num_th):\n",
    "    # find the points that minimizes the variance of the data minus spline interpolation\n",
    "    # scan all points, who cares\n",
    "    idx_left=list(set(range(res)) - set(peak_pos))\n",
    "    while False:\n",
    "        plt.plot(x_data,y_data)\n",
    "        plt.plot(x_data[idx_left],y_data[idx_left],'*')\n",
    "    interpol_pos = []\n",
    "    interpol_pos =  [0,np.argmin(y_data[idx_left]),res-1]\n",
    "    #for i in range(int(y_data.shape[0]/slide_win)):\n",
    "    for i in range(2*int(y_data.shape[0]/slide_win)):\n",
    "        min_pos=0\n",
    "        y_hat = si.interp1d(x_data[interpol_pos],y_data[interpol_pos])(x_data)\n",
    "        min_mse=(1-nu)*np.dot((y_hat-y_data>0),np.abs((y_hat-y_data)))+nu*np.var(y_hat-y_data)\n",
    "        tmp_interpol_pos=list(set(range(int(i*(slide_win/2)),min(int(i*(slide_win/2)+(slide_win)),res))) - set(peak_pos))\n",
    "        for k in range(len(tmp_interpol_pos)):\n",
    "            tmp_pos=np.concatenate((interpol_pos,[tmp_interpol_pos[k]]))\n",
    "            y_hat = si.interp1d(x_data[tmp_pos],y_data[tmp_pos])(x_data)\n",
    "            # generalize to any loss\n",
    "            # tmp_mse=np.var(y_hat-y_data)\n",
    "            tmp_mse=(1-nu)*np.dot((y_hat-y_data>0),np.abs((y_hat-y_data)))+nu*np.var(y_hat-y_data)\n",
    "            # update the minimum\n",
    "            if tmp_mse<min_mse:\n",
    "                min_pos = tmp_interpol_pos[k]\n",
    "                min_mse = tmp_mse\n",
    "        interpol_pos.append(min_pos)\n",
    "        #interpol_pos.sort()\n",
    "        interpol_mse.append(min_mse)\n",
    "        unique_pos=np.array([int(interpol_pos.index(x)) for x in set(interpol_pos)])\n",
    "        interpol_pos=list(np.array(interpol_pos)[unique_pos.astype(int)])\n",
    "        interpol_mse=list(np.array(interpol_mse)[unique_pos.astype(int)])\n",
    "        # sort points\n",
    "        sort_pos=np.argsort(interpol_pos)\n",
    "        interpol_pos=list(np.array(interpol_pos)[sort_pos.astype(int)])\n",
    "        interpol_mse=list(np.array(interpol_mse)[sort_pos.astype(int)])\n",
    "        # remove points that are too close\n",
    "    y_hat = si.interp1d(x_data[interpol_pos],y_data[interpol_pos])(x_data)\n",
    "    y_bsp = np.poly1d(np.polyfit(x_data[interpol_pos], y_data[interpol_pos], 3) )(x_data)\n",
    "    #y_bsp=si.interp1d(x_data[interpol_pos],y_data[interpol_pos], kind='cubic')(x_data)\n",
    "    while True:\n",
    "        plt.plot(x_data,y_data)\n",
    "        plt.plot(x_data,y_hat)\n",
    "        plt.plot(x_data,y_bsp)\n",
    "    mean_level=y_bsp\n",
    "    #--------------------------------------------------------------------\n",
    "    #   find when you acre over the mean level\n",
    "    #--------------------------------------------------------------------\n",
    "    # th=2*np.sqrt(np.var(y_data-mean_level))\n",
    "    th=np.sqrt(np.var(y_data-mean_level))\n",
    "    pos=np.array(np.where((y_data-mean_level)<th)[0])\n",
    "    while True:\n",
    "        plt.plot(x_data,y_data)\n",
    "        plt.plot(x_data,y_bsp)\n",
    "        plt.plot(x_data[pos],y_bsp[pos],'*')\n",
    "    #--------------------------------------------------------------------\n",
    "    #   merge the points\n",
    "    #--------------------------------------------------------------------\n",
    "    diff_pos=pos[1:]-pos[:-1]-1\n",
    "    jumps=np.where(diff_pos>0)[0]\n",
    "    #if the final element is res, the add a jomp an the end\n",
    "    if pos[-1]==res-1:\n",
    "        jumps=np.append(jumps,pos.shape[0]-1)\n",
    "    final_lb=[]\n",
    "    final_rb=[]\n",
    "    if jumps.size==0:\n",
    "            final_lb.append(pos[0])\n",
    "            final_rb.append(pos[-1])\n",
    "    else:\n",
    "            final_lb.append(pos[0])\n",
    "            final_rb.append(pos[jumps[0]])\n",
    "            k=0\n",
    "            while k<jumps.shape[0]-1:\n",
    "                #\n",
    "                final_lb.append(pos[jumps[k]+1])\n",
    "                # go to the next gap\n",
    "                k=k+1\n",
    "                final_rb.append(pos[jumps[k]])\n",
    "    # add the first and the last intervals\n",
    "    idx_lr=np.zeros([2,len(final_rb)])\n",
    "    idx_lr[0]=np.array(final_lb)\n",
    "    idx_lr[1]=np.array(final_rb)\n",
    "    idx_lr.astype(int)\n",
    "    idx_lr=idx_lr.T\n",
    "    # merge intervals\n",
    "    # remove the one intervals\n",
    "    idx_lr=idx_lr[np.where(idx_lr[:,1]-idx_lr[:,0]>2)[0],:]\n",
    "    merged = recursive_merge(idx_lr.tolist())\n",
    "    idx_lr = np.array(merged).astype(int)\n",
    "    idx_lr_poly = idx_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_est_florescence(f_sup,data_sub):\n",
    "    \n",
    "    #min_data_amm=np.min(data_sub,axis=1)\n",
    "    if data_sub.ndim>1:\n",
    "        min_data_amm=np.mean(data_sub, axis = 0)\n",
    "    else:\n",
    "        min_data_amm=data_sub\n",
    "    \n",
    "    poly_min=np.poly1d(np.polyfit(f_sup,min_data_amm,3))(f_sup)\n",
    "    poly_min_pos=poly_min+min(min_data_amm-poly_min)\n",
    "    \n",
    "    beta_init=np.polyfit(f_sup,min_data_amm,4)\n",
    "    beta_init[4]=beta_init[4]+min(min_data_amm-poly_min)\n",
    "    \n",
    "    result = minimize(pos_mse_loss, beta_init, args=(f_sup,min_data_amm,poly4), method='Nelder-Mead', tol=1e-12)\n",
    "#     result = minimize(mse_loss, beta_init, args=(f_sup,min_data_amm,poly4), method='Nelder-Mead', tol=1e-12)\n",
    "#     result = minimize(reg_pos_mse_loss, beta_init, args=(f_sup,min_data_amm,poly4), method='Nelder-Mead', tol=1e-12)\n",
    "    \n",
    "    beta_hat = result.x                 \n",
    "        \n",
    "    plt_back=0\n",
    "        \n",
    "    if plt_back:\n",
    "            plt.figure('Pos MSE fit')\n",
    "            #plt.plot(f_sup,np.mean(data_sub,axis=1),'--',label='original data')    \n",
    "            plt.plot(f_sup,min_data_amm,label='original data')    \n",
    "            #plt.plot(f_sup,np.mean(data_sub,axis=1),'-.',label='data minus bias')\n",
    "            plt.plot(f_sup,poly4(f_sup,beta_init),'-.',label='poly 4 init')\n",
    "            plt.plot(f_sup,poly4(f_sup,beta_hat),'-.',label='poly 4 hat')\n",
    "            plt.legend()\n",
    "            \n",
    "        \n",
    "    # subtract mean\n",
    "    \n",
    "    if data_sub.ndim>1:\n",
    "        data_sub2=np.subtract(data_sub,poly4(f_sup,beta_hat).reshape([data_sub.shape[0],1]))\n",
    "    else:\n",
    "        data_sub2=data_sub-poly4(f_sup,beta_hat)\n",
    "            \n",
    "    plt_final=0\n",
    "    \n",
    "    if plt_final:\n",
    "        plt.figure('Final recap')        \n",
    "        plt.plot(f_sup,poly4(f_sup,beta_hat),'--',label='poly 4 hat')\n",
    "        plt.plot(f_sup,min_data_amm,label='original data-back')    \n",
    "        if data_sub.ndim>1:\n",
    "            plt.plot(f_sup,np.mean(data_sub2,axis=1),label='original data-back-pos_MSE')    \n",
    "        else:\n",
    "            plt.plot(f_sup,data_sub2,label='original data-back-pos_MSE')    \n",
    "        \n",
    "        plt.legend()\n",
    "        \n",
    "    return data_sub2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_data(y_data,win):\n",
    "    smooth=np.array([np.mean(y_data[int(np.max([j-win,0])):int(np.min([j+win,y_data.size]))]) for j in  range(y_data.shape[0])])\n",
    "    return smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positive_mse(y_pred,y_true):                \n",
    "    loss=np.dot((10**8*(y_pred-y_true>0)+np.ones(y_true.size)).T,(y_pred-y_true)**2)\n",
    "    return sum(loss)/y_true.size\n",
    "\n",
    "def pos_mse_loss(beta, X, Y,function):                \n",
    "    return positive_mse(function(X,beta), Y)\n",
    "\n",
    "\n",
    "def positive_mse_loss(beta, X, Y):\n",
    "    p=np.poly1d(beta)\n",
    "    error = sum([positive_mse(p(X[i]), Y[i]) for i in range(X.size)])/X.size\n",
    "    return(error)\n",
    "\n",
    "def reg_positive_mse(y_pred,y_true):                \n",
    "    loss_pos=(y_pred-y_true)**2\n",
    "    loss_neg=np.dot((y_pred-y_true<0),(y_pred-y_true)**8)\n",
    "    loss=loss_pos+loss_neg\n",
    "    return np.sum(loss)/y_true.size\n",
    "\n",
    "def reg_pos_loss(beta, X, Y,function):                \n",
    "    return positive_mse(function(X,beta), Y)\n",
    "\n",
    "\n",
    "def mse_loss(beta,X, Y,function):\n",
    "    return sum((function(X,beta)-Y)**2)/Y.size\n",
    "\n",
    "def recursive_merge(inter, start_index = 0):\n",
    "    for i in range(start_index, len(inter) - 1):\n",
    "        if inter[i][1] > inter[i+1][0]:\n",
    "            new_start = inter[i][0]\n",
    "            new_end = inter[i+1][1]\n",
    "            inter[i] = [new_start, new_end]\n",
    "            del inter[i+1]\n",
    "            return recursive_merge(inter.copy(), start_index=i)\n",
    "    return inter    \n",
    "\n",
    "def poly4(x_data,beta):\n",
    "        p=np.poly1d(beta)\n",
    "        return p(x_data)\n",
    "        \n",
    "def positive_mse(y_pred,y_true):                \n",
    "    loss=np.dot((10**8*(y_pred-y_true>0)+np.ones(y_true.size)).T,(y_pred-y_true)**2)\n",
    "    return np.sum(loss)/y_true.size\n",
    "\n",
    "def pos_mse_loss(beta, X, Y,function):                \n",
    "    return positive_mse(function(X,beta), Y)\n",
    "\n",
    "def reg_positive_mse(y_pred,y_true):                \n",
    "    loss_pos=(y_pred-y_true)**2\n",
    "    loss_neg=np.dot((y_pred-y_true>0),np.abs((y_pred-y_true)))\n",
    "    loss=loss_pos+loss_neg\n",
    "    return np.sum(loss)/y_true.size\n",
    "\n",
    "def reg_pos_mse_loss(beta, X, Y,function):                \n",
    "    return reg_positive_mse(function(X,beta), Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import pristine spectrum\n",
    "Prisitine spectra will be regarded as spectrum of substrate itself without analytes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pristine_area1 = pd.read_csv('area1_Exported.dat', sep=\"\\s+\", header=None)\n",
    "\n",
    "length = pristine_area1.shape[1] - 1\n",
    "f_sup_pristine = np.array(pristine_area1.iloc[:, 0], dtype=np.float64)\n",
    "pristine_area1 = np.array(pristine_area1.iloc[:, 1], dtype=np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import MG of different concetrations\n",
    "This type of import works in Jupyter, however doesn't work in Spyder or Pycharm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename='shelve_save_data.out'\n",
    "\n",
    "my_shelf = shelve.open(filename)\n",
    "# klist = list(my_shelf.keys())\n",
    "for key in my_shelf:\n",
    "    globals()[key] = my_shelf[key]\n",
    "my_shelf.close()\n",
    "# print(klist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Save the wavelength variable \n",
    "f_sup_pure = np.copy(f_sup)\n",
    "f_sup_pure.shape\n",
    "# print(klist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import mixed analytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename='shelve_save_data_analyte.out'\n",
    "\n",
    "my_shelf = shelve.open(filename)\n",
    "for key in my_shelf:\n",
    "    globals()[key]=my_shelf[key]\n",
    "my_shelf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the wavelength variable for mixed\n",
    "f_sup_mix = np.copy(f_sup)\n",
    "f_sup_mix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data for different batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import MG from file to variable \n",
    "Then assign different slices to different conetrations. We also encode labels to one-hot vector and get rid of zero spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_data_pure = np.zeros((30000, 1600))\n",
    "# print(whole_data.shape)\n",
    "whole_data_pure[0:2500] = np.copy(data_15000[0])\n",
    "whole_data_pure[2500:5000] = np.copy(data_15000[1])\n",
    "whole_data_pure[5000:7500] = np.copy(data_15000[2])\n",
    "whole_data_pure[7500:10000] = np.copy(data_15000[3])\n",
    "whole_data_pure[10000:12500] = np.copy(data_1500[0])\n",
    "whole_data_pure[12500:15000] = np.copy(data_1500[1])\n",
    "whole_data_pure[15000:17500] = np.copy(data_1500[2])\n",
    "whole_data_pure[17500:20000] = np.copy(data_1500[3])\n",
    "whole_data_pure[20000:22500] = np.copy(data_150[0])\n",
    "whole_data_pure[22500:25000] = np.copy(data_150[1])\n",
    "whole_data_pure[25000:27500] = np.copy(data_150[2])\n",
    "whole_data_pure[27500:30000] = np.copy(data_150[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One hot vector works for Keras, but for Stanford Pytorch model class labels should be converted to integers.  \n",
    "`mask` variable is needed for eliminating empty spectra "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anvarkunanbaev/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "training_data = np.copy(whole_data_pure) / np.max(whole_data_pure, axis=-1, keepdims=True)\n",
    "mask = np.all(np.isfinite(training_data), axis=-1)\n",
    "# mask = np.all(training_data, axis=-1)\n",
    "labels = np.zeros((30000, 3))\n",
    "labels[:10000, 0] = 1\n",
    "labels[10000:20000, 1] = 1\n",
    "labels[20000:, 2] = 1\n",
    "# Remove the following line if need to run in Keras\n",
    "labels_not_one_hot = np.where(labels==1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True  True ... False False False]\n"
     ]
    }
   ],
   "source": [
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25511, 1600)\n",
      "(25511,)\n"
     ]
    }
   ],
   "source": [
    "X = training_data[mask]\n",
    "y = labels_not_one_hot[mask]\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pretreatment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FFT and Bandpass approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_orig = np.zeros(2048,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_orig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_orig = np.copy(X[6, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(signal_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_orig_smooth = smooth_data(signal_orig, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_ste_poly = remove_est_florescence(f_sup_pure, signal_orig)\n",
    "signal_ste_poly.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(f_sup_pure, (signal_orig_smooth-poly_interp)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_orig = np.copy(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_ste_poly = np.zeros_like(signal_orig)\n",
    "for i in range(signal_orig.shape[0]):\n",
    "    signal_ste_poly[i, :] = remove_est_florescence(f_sup_pure, signal_orig[i, :])\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename='shelve_save_data.out'\n",
    "    \n",
    "my_shelf = shelve.open(filename)\n",
    "\n",
    "my_shelf[\"data_rm_fl\"]=signal_ste_poly \n",
    "\n",
    "my_shelf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_ste_smooth = smooth_data(signal_ste_poly, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(signal_ste_smooth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Two ways of modifying signals  \n",
    "1st Concatenate the mirrored signal to the end of the original signal  \n",
    "\n",
    "2nd Zero pad so that the number of points in signal is a power of 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(signal_orig))\n",
    "print(type(np.flip(signal_orig)))\n",
    "signal_orig_hstacked = np.hstack((signal_ste_poly, np.fliplr(signal_ste_poly)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_orig_hstacked = np.hstack((signal_orig, np.flip(signal_orig)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_orig_hstacked = np.hstack((signal_orig, np.fliplr(signal_orig)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(signal_orig_hstacked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_spec = np.fft.fftshift(np.fft.fft(signal_orig_hstacked), axes=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying something new \n",
    "signal_spec = np.fft.fft(signal_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(signal_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_spec = np.fft.fft(signal_orig)\n",
    "# print(signal_spec[0:16])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply highpass and lowpass filter to the FFT of the signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = X.shape[-1]\n",
    "print(N)\n",
    "first_segment_hp = np.array(range(0, int(0.005*N)))\n",
    "second_segment_hp = np.array(range(int(0.005*N), int(0.01*N)))\n",
    "third_segment_hp = np.array(range(int(0.01*N), N))\n",
    "first_segment_lp = np.array(range(0, int(0.25*N)))\n",
    "second_segment_lp = np.array(range(int(0.25*N), int(0.5*N)))\n",
    "third_segment_lp = np.array(range(int(0.5*N), N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Not concatenated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Highpass\n",
    "signal_spec[first_segment_hp] = signal_spec[first_segment_hp] * ((first_segment_hp * 2) / N)\n",
    "signal_spec[second_segment_hp] = signal_spec[second_segment_hp] * \\\n",
    "                                    (((second_segment_hp - 0.005 * N) / N) * (0.89/0.005))\n",
    "signal_spec[third_segment_hp] = signal_spec[third_segment_hp]* \\\n",
    "                                     (((third_segment_hp - 0.01 * N) / N) * (0.1/0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowpass \n",
    "signal_spec[first_segment_lp] = signal_spec[first_segment_lp] * (1 - (first_segment_lp * 0.1 / (0.25 * N)))\n",
    "signal_spec[second_segment_lp] = signal_spec[second_segment_lp] * \\\n",
    "                                (1.7 - ((second_segment_lp * 0.8) / (0.25 * N)))\n",
    "signal_spec[third_segment_lp] = signal_spec[third_segment_lp] * \\\n",
    "                                (0.2 - ((third_segment_lp * 0.1) / (0.5 * N)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenated "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Highpass filtering\n",
    "signal_spec[N + first_segment_hp] = signal_spec[N + first_segment_hp] * ((first_segment_hp * 2) / N)\n",
    "signal_spec[N - first_segment_hp] = signal_spec[N - first_segment_hp] * ((first_segment_hp * 2) / N)\n",
    "signal_spec[N + second_segment_hp] = signal_spec[N + second_segment_hp] * \\\n",
    "                                    (((second_segment_hp - 0.005 * N) / N) * (0.89/0.005))\n",
    "signal_spec[N - second_segment_hp] = signal_spec[N - second_segment_hp] * \\\n",
    "                                    (((second_segment_hp - 0.005 * N) / N) * (0.89/0.005))\n",
    "signal_spec[N + third_segment_hp] = signal_spec[N + third_segment_hp]* \\\n",
    "                                     (((third_segment_hp - 0.01 * N) / N) * (0.1/0.99))\n",
    "signal_spec[N - third_segment_hp] = signal_spec[N - third_segment_hp] * \\\n",
    "                                    (((third_segment_hp - 0.01 * N) / N) * (0.1/0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowpass filtering\n",
    "signal_spec[N + first_segment_lp] = signal_spec[N + first_segment_lp] * (1 - (first_segment_lp * 0.1 / (0.25 * N)))\n",
    "signal_spec[N - first_segment_lp] = signal_spec[N - first_segment_lp] * (1 - (first_segment_lp * 0.1 / (0.25 * N)))\n",
    "signal_spec[N + second_segment_lp] = signal_spec[N + second_segment_lp] * \\\n",
    "                                (1.7 - ((second_segment_lp * 0.8) / (0.25 * N)))\n",
    "signal_spec[N - second_segment_lp] = signal_spec[N - second_segment_lp] * \\\n",
    "                                (1.7 - ((second_segment_lp * 0.8) / (0.25 * N)))\n",
    "signal_spec[N + third_segment_lp] = signal_spec[N + third_segment_lp] * \\\n",
    "                                (0.2 - ((third_segment_lp * 0.1) / (0.5 * N)))\n",
    "signal_spec[N - third_segment_lp] = signal_spec[N - third_segment_lp] * \\\n",
    "                                (0.2 - ((third_segment_lp * 0.1) / (0.5 * N)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowpass filtering\n",
    "signal_spec[:, N + first_segment_lp] = signal_spec[:, N + first_segment_lp] * (1 - (first_segment_lp * 0.1 / (0.25 * N)))\n",
    "signal_spec[:, N - first_segment_lp] = signal_spec[:, N - first_segment_lp] * (1 - (first_segment_lp * 0.1 / (0.25 * N)))\n",
    "signal_spec[:, N + second_segment_lp] = signal_spec[:, N + second_segment_lp] * \\\n",
    "                                (1.7 - ((second_segment_lp * 0.8) / (0.25 * N)))\n",
    "signal_spec[:, N - second_segment_lp] = signal_spec[:, N - second_segment_lp] * \\\n",
    "                                (1.7 - ((second_segment_lp * 0.8) / (0.25 * N)))\n",
    "signal_spec[:, N + third_segment_lp] = signal_spec[:, N + third_segment_lp] * \\\n",
    "                                (0.2 - ((third_segment_lp * 0.1) / (0.5 * N)))\n",
    "signal_spec[:, N - third_segment_lp] = signal_spec[:, N - third_segment_lp] * \\\n",
    "                                (0.2 - ((third_segment_lp * 0.1) / (0.5 * N)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# signal_spec[np.where(signal_spec > 10)] = 0.8\n",
    "# signal_spec[np.where(signal_spec < -10)] = -0.8\n",
    "signal_spec[1585:1616] = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.where(signal_spec > 0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.max(signal_spec)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(signal_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_spec = butter_bandpass_filter(signal_spec, lowcut, highcut, fs, order=order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# signal_spec[signal_spec < 40] = 0\n",
    "signal_ifftd = np.fft.ifft(np.fft.ifftshift(signal_spec, axes=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying something new\n",
    "signal_ifftd = np.fft.ifft(40 * signal_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_ifftd = np.fft.ifft(signal_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_ifft_reg_pos_mse_loss = np.copy(signal_ifftd[0:1600])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_ifft_pos_mse_loss = np.copy(signal_ifftd[0:1600])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_ifft_mse_loss = np.copy(signal_ifftd[0:1600])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_ifft = np.copy(signal_ifftd[:, 0:1600] / np.max(signal_ifftd, axis=-1, keepdims=True))\n",
    "sig_ifft[:, np.where(sig_ifft < 0)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_ifftd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,18))\n",
    "plt.plot(range(0, 1600), signal_ifftd[0:1600]**2, label='IFFT')\n",
    "plt.plot(range(0, 1600), signal_orig[0:1600], label='Orig')\n",
    "plt.plot(range(0, 1600), signal_orig[0:1600] - (signal_ifftd[0:1600] ** 2), label='smoothed & removed')\n",
    "# plt.plot(range(0, 1600), signal_ste_poly, label='Ste rm fl')\n",
    "# plt.plot(range(0,1600), signal_orig_smooth, label='Smooth')\n",
    "# plt.plot(range(0, 1600), signal_orig[0:1600] - (signal_ifftd[0:1600] / np.max(signal_ifftd)), label='Removed')\n",
    "# plt.plot(range(0, 1600), (signal_orig - check)[0:1600])\n",
    "plt.legend()\n",
    "# plt.plot(range(0,3200), signal_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,18))\n",
    "plt.plot(range(0, 1600), signal_orig+0.2, label='Orig')\n",
    "plt.plot(range(0, 1600), sig_ifft_reg_pos_mse_loss, label='Reg pos mse')\n",
    "plt.plot(range(0, 1600), sig_ifft_mse_loss, label='mse')\n",
    "plt.plot(range(0, 1600), sig_ifft_pos_mse_loss+0.2, label='pos mse')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_ifft_pos_mse_loss == sig_ifft_reg_pos_mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_th=100\n",
    "poly_interp = np.poly1d(np.polyfit(f_sup_pure,signal_ifftd[0:1600],5))(f_sup_pure)\n",
    "idx_pos_min = np.argsort((signal_ifftd[0:1600]-poly_interp)**2)[:num_th]\n",
    "\n",
    "y_interp = np.interp(idx_pos_min, f_sup_pure, signal_orig)\n",
    "signal_processed = (signal_ifftd[0:1600]-poly_interp)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial fitting and background removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_th=100\n",
    "x_data = np.copy(f_sup_pure)\n",
    "y_data = np.copy(X[6, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for signal \n",
    "signal_stacked = np.hstack((y_data, np.flip(y_data)))\n",
    "W = np.fft.fftshift(np.fft.fftfreq(signal_stacked.size, d = x_data[1] - x_data[0]) * (x_data[1] - x_data[0]))\n",
    "print(np.mean((W*8000)/2*np.pi))\n",
    "# signal_fft = np.fft.fftshift(np.fft.fft(signal_stacked), axes=-1)\n",
    "signal_fft = np.fft.fft(signal_stacked)\n",
    "# signal_fft[(W < 500)] = 0\n",
    "# signal_fft[(W > 1500)] = 0\n",
    "# signal_ifftd = np.fft.ifft(np.fft.ifftshift(signal_fft, axes=-1))\n",
    "signal_ifftd = np.fft.ifft(signal_fft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single signal example \n",
    "# for proof of concept\n",
    "poly_interp = np.poly1d(np.polyfit(x_data,y_data,5))(x_data)\n",
    "idx_pos_min = np.argsort((y_data-poly_interp)**2)[:num_th]\n",
    "\n",
    "y_interp = np.interp(idx_pos_min, f_sup_pure, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General \n",
    "poly_interp = np.poly1d(np.polyfit(x_data,y_data,4))(x_data)\n",
    "idx_pos_min = np.argsort((y_data-poly_interp)**2)[:num_th]\n",
    "\n",
    "y_interp = np.interp(idx_pos_min, f_sup_pure, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.vstack([x_data, np.ones(len(x_data))]).T\n",
    "m, c = np.linalg.lstsq(A, y_data, rcond=None)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spl = UnivariateSpline(x_data, y_data, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.plot(x_data,y_data)\n",
    "plt.plot(x_data, signal_ifftd[0:1600])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(f_sup_pure, np.abs(y_data - poly_interp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtaining signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_with_polyfit = y_data - poly_interp\n",
    "signal_with_polyfit_hstacked = np.hstack((signal_with_polyfit, np.flip(signal_with_polyfit)))\n",
    "signal_with_polyfit_fft = np.fft.fftshift(np.fft.fft(signal_with_polyfit_hstacked))\n",
    "signal_with_polyfit_filtered_fft = butter_bandpass_filter(signal_with_polyfit_fft, lowcut, highcut, fs, order=order)\n",
    "signal_with_polyfit_filtered_ifft = np.fft.ifft(np.fft.ifftshift(signal_with_polyfit_filtered_fft))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal = np.hstack((y_data, np.flip(y_data)))\n",
    "signal_fft = np.fft.fftshift(np.fft.fft(flu_signal), axes=-1)\n",
    "signal_fft[:1400] = 0\n",
    "signal_fft[1500:] = 0\n",
    "ifftd = np.fft.ifft(np.fft.ifftshift(flu_signal_fft, axes=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtaining fluoresence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flu_signal = np.hstack((y_data, np.flip(y_data)))\n",
    "flu_signal_fft = np.fft.fftshift(np.fft.fft(flu_signal), axes=-1)\n",
    "flu_signal_fft[500:] = 0\n",
    "flu_ifftd = np.fft.ifft(np.fft.ifftshift(flu_signal_fft, axes=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtaining noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_signal = np.hstack((y_data, np.flip(y_data)))\n",
    "noise_signal_fft = np.fft.fftshift(np.fft.fft(noise_signal), axes=-1)\n",
    "noise_signal_fft[:1500] = 0\n",
    "noise_ifftd = np.fft.ifft(np.fft.ifftshift(noise_signal_fft, axes=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting all three together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(x_data,y_data)\n",
    "# plt.plot(x_data, flu_ifftd[0:1600])\n",
    "# plt.plot(x_data, noise_ifftd[0:1600])\n",
    "# plt.plot(x_data, ifftd[0:1600])\n",
    "# plt.plot(x_data, flu_ifftd[0:1600]+noise_ifftd[0:1600]+ifftd[0:1600])\n",
    "plt.plot(x_data, np.abs(signal_with_polyfit_filtered_ifft[0:1600]))\n",
    "# plt.plot(f_sup_pure, np.abs(y_data - poly_interp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store the pretreated data for further training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This one is for the original data without any pretreatment \n",
    "idx = np.random.permutation(len(X))\n",
    "\n",
    "X = np.copy(X)[idx]\n",
    "y = np.copy(y)[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This one is for the data that was pretreated with FFT and Butterworth bandpass\n",
    "idx = np.random.permutation(len(X))\n",
    "\n",
    "X = np.copy(signal_ifftd[:, 0:1600])[idx]\n",
    "y = np.copy(y)[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This one is for the data that was pretreated with FFT and Butterworth bandpass\n",
    "idx = np.random.permutation(len(X))\n",
    "\n",
    "X = np.copy(sig_ifft)[idx]\n",
    "y = np.copy(y)[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_20, y_train_20 = X[0:5000], y[0:5000]\n",
    "x_train_30, y_train_30 = X[0:7500], y[0:7500]\n",
    "x_train_40, y_train_40 = X[0:10000], y[0:10000]\n",
    "x_train_50, y_train_50 = X[0:12500], y[0:12500]\n",
    "x_train_60, y_train_60 = X[0:15000], y[0:15000]\n",
    "x_train_70, y_train_70 = X[0:17500], y[0:17500]\n",
    "x_train_80, y_train_80 = X[0:20000], y[0:20000]\n",
    "\n",
    "print(x_train_20.shape, y_train_20.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_class_1 = np.array(np.where(y_train_20[:] == 0))[0]\n",
    "mask_class_2 = np.array(np.where(y_train_20[:] == 1))[0]\n",
    "mask_class_3 = np.array(np.where(y_train_20[:] == 2))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_1_average = np.sum(x_train_20[mask_class_1], axis=0) / len(mask_class_1)\n",
    "class_2_average = np.sum(x_train_20[mask_class_2], axis=0) / len(mask_class_2)\n",
    "class_3_average = np.sum(x_train_20[mask_class_3], axis=0) / len(mask_class_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,18))\n",
    "\n",
    "plt.plot(f_sup_pure, np.abs(class_1_average), label=\"15000\")\n",
    "# plt.plot(f_sup_pure, class_1_average, label=\"15000 not smoothed\")\n",
    "plt.plot(f_sup_pure, np.abs(class_2_average), label=\"1500\")\n",
    "plt.plot(f_sup_pure, np.abs(class_3_average), label=\"150 \")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare test data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_20, y_test_20 = X[5000:6250], y[5000:6250]\n",
    "x_test_30, y_test_30 = X[7500:9375], y[7500:9375]\n",
    "x_test_40, y_test_40 = X[10000:12500], y[10000:12500]\n",
    "x_test_50, y_test_50 = X[10000:12500], y[10000:12500]\n",
    "x_test_60, y_test_60 = X[12500:15625], y[12500:15625]\n",
    "x_test_70, y_test_70 = X[17500:22500], y[17500:22500]\n",
    "x_test_80, y_test_80 = X[20000:], y[20000:]\n",
    "\n",
    "print(x_test_20.shape, y_test_20.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
